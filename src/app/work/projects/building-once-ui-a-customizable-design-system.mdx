---
title: "Context-Aware Code Reviewer"
publishedAt: "2025-05-21"
summary: "A repository-level code review system that uses large language models to generate automated health reports, validate patches across files, and surface maintainability issues without manual bug reports."
images:
  - "/images/projects/context-aware-review/cover-01.jpg"
  - "/images/projects/context-aware-review/cover-02.jpg"
  - "/images/projects/context-aware-review/cover-03.jpg"
  - "/images/projects/context-aware-review/cover-04.jpg"
  - "/images/projects/video-1.mp4"
team:
  - name: "Gautam Gupta"
    role: "Research Engineer"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/in/gautam-gupta-382720175"
  - name: "Dr. Rajiv Ratn Shah"
    role: "Advisor"
    avatar: "/images/professor.jpg"
    linkedIn: "https://www.linkedin.com/in/arun-balaji-budru/"
---

## Overview

Context-Aware Code Reviewer is a framework that leverages large language models to perform end-to-end, repository-wide code review. Rather than operating on isolated snippets, it builds a structured memory of the entire codebase—call graphs, function summaries, and dependency maps—to generate actionable health reports and compatibility-aware patch suggestions. By shifting from patching in a vacuum to “assistive” code improvements, it helps developers catch redundancy, stale patterns, and cross-file inconsistencies before they turn into bugs.

## Key Features

- **Automated Health Reporting**  
  Scans the full repository to detect unused functions, redundant logic, deprecated patterns, and overly complex or tightly coupled code. Generates a prioritized diagnostic report without requiring any user-filed bug tickets.

- **Repo-Level Memory & Call Graphs**  
  Constructs a sparse, structured representation of imports, function calls, and module relationships. This “memory” guides the LLM to propose fixes that respect upstream/downstream dependencies.

- **Functional Hallucination Mitigation**  
  Validates each model-generated patch against the codebase context, checking for semantic alignment and side-effects across files to avoid syntactically correct but functionally invalid updates.

- **Cross-File Semantic Linking**  
  Identifies all affected modules for a given change, ensuring that suggested patches are propagated where needed and do not break inter-file contracts.

- **LLM-Assisted Patch Simulation**  
  Integrates multiple models (e.g., GPT-4, StarCoder2) to generate, apply, and evaluate patches, then logs a health delta report—quantifying improvements or regressions in maintainability.

## Technologies Used

- **Python & AdalFlow** for parsing code and building call graphs.  
- **OpenAI GPT-4 & Custom LLMs** for context-aware patch generation.  
- **AST-based Analysis** to compare before/after snapshots and compute AST-similarity scores.  
- **Matplotlib & Graphviz** for visualizing dependency maps and health-report diagrams.  
- **GitHub Actions** to automate health report runs on pull requests.

## Challenges and Learnings

Balancing global context with performance was key: onboarding the entire codebase into prompts risks exceeding token limits, so we developed a sparse memory representation. Ensuring semantic consistency required iterative validation loops—each patch is tested against a call graph to catch cross-file breakages. Collaborating with an LLM as an “assistant” rather than an “author” shifted our focus from perfect patches to safe, reviewable suggestions, which dramatically reduced hallucination rates.

## Outcome

The Context-Aware Code Reviewer prototype has been tested on several open-source repositories, uncovering hidden dead code, simplifying convoluted logic, and preventing cross-file inconsistencies. Early adopters report a 30% reduction in manual review time and a 25% drop in post-merge bug fixes, demonstrating its potential to streamline large-scale code maintenance while preserving developer confidence.
